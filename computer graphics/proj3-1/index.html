<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">

<head>
  <style>
    body {
      background-color: white;
      padding: 100px;
      width: 1000px;
      margin: auto;
      text-align: left;
      font-weight: 300;
      font-family: 'Open Sans', sans-serif;
      color: #121212;
    }

    h1,
    h2,
    h3,
    h4 {
      font-family: 'Source Sans Pro', sans-serif;
    }

    kbd {
      color: #121212;
    }
  </style>
  <title>CS 184 Path Tracer</title>
  <meta http-equiv="content-type" content="text/html; charset=utf-8" />
  <link href="https://fonts.googleapis.com/css?family=Open+Sans|Source+Sans+Pro" rel="stylesheet">

  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      }
    };
  </script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
  </script>

</head>


<body>

  <h1 align="middle">CS 184: Computer Graphics and Imaging, Spring 2023</h1>
  <h1 align="middle">Project 3-1: Path Tracer</h1>
  <h2 align="middle"></h2>

  <!-- Add Website URL -->
  <!-- <h2 align="middle">Website URL: <a href="TODO">TODO</a></h2> -->

  <br><br>


  <!-- <div align="center">
  <table style="width=100%">
      <tr>
          <td align="middle">
          <img src="images/example_image.png" width="480px" />
          <figcaption align="middle">Results Caption: my bunny is the bounciest bunny</figcaption>
      </tr>
  </table>
</div> -->

  <!-- <p>All of the text in your write-up should be <em>in your own words</em>. If you need to add additional HTML features to this document, you can search the <a href="http://www.w3schools.com/">http://www.w3schools.com/</a> website for instructions. To edit the HTML, you can just copy and paste existing chunks and fill in the text and image file names appropriately.</p>
<o>The website writeup is intended to be a self-contained walkthrough of the assignment: we want this to be a piece of work which showcases your understanding of relevant concepts through both mesh images as well as written explanations about what you did to complete each part of the assignment. Try to be as clear and organized as possible when writing about your own output files or extensions to the assignment. We want to understand what you've achieved and how you've done it!</p> 
<p>If you are well-versed in web development, feel free to ditch this template and make a better looking page.</p>


<p>Here are a few problems students have encountered in the past. Test your website on the instructional machines early!</p>
<ul>
<li>Your main report page should be called index.html.</li>
<li>Be sure to include and turn in all of the other files (such as images) that are linked in your report!</li>
<li>Use only <em>relative</em> paths to files, such as <pre>"./images/image.jpg"</pre>
Do <em>NOT</em> use absolute paths, such as <pre>"/Users/student/Desktop/image.jpg"</pre></li>
<li>Pay close attention to your filename extensions. Remember that on UNIX systems (such as the instructional machines), capitalization matters. <pre>.png != .jpeg != .jpg != .JPG</pre></li>
<li>Be sure to adjust the permissions on your files so that they are world readable. For more information on this please see this tutorial: <a href="http://www.grymoire.com/Unix/Permissions.html">http://www.grymoire.com/Unix/Permissions.html</a></li>
<li>And again, test your website on the instructional machines early!</li>
</ul>


<p>Here is an example of how to include a simple formula:</p>
<p align="middle"><pre align="middle">a^2 + b^2 = c^2</pre></p>
<p>or, alternatively, you can include an SVG image of a LaTex formula.</p> -->

  <div>

    <h2 align="middle">Overview</h2>
    <p>
      In this project, we have methodically implemented several key components: Ray Generation and Scene Intersection,
      Bounding Volume Hierarchy, Direct Illumination, Global Illumination, and Adaptive Sampling.
      <br><br>
      In Ray Generation and Scene Intersection, we implemented Generating Camera Rays and Generating Pixel Samples,
      alongside Ray-Triangle and Ray-Sphere Intersection algorithms. Our method starts by generating the equation of the
      ray in camera space and transforming it into world space. We then estimate the radiance of a pixel by averaging
      'ns_aa' samples and test the ray-primitive intersections, combining the ray equations with the geometries of
      triangles and spheres.
      <br><br>
      In Bounding Volume Hierarchy, our approach to constructing the BVH involved a recursive process, using the
      x-coordinate average centroid of primitives for splitting. We developed methods for Intersecting the Bounding Box
      and the BVH itself. Challenges like stack overflow due to improper partitioning were tackled, and we improved the
      bounding box intersection conditions for more accurate results.
      <br><br>
      In Direct Illumination, we implemented Diffuse BSDF and Zero-bounce Illumination, and explored Direct Lighting
      using both Uniform Hemisphere Sampling and Importance Sampling Lights. We returned the reflection divided by PI as
      f(wi -> wo) and computed the zero-bounce radiance. Our primary sampling methods involved a Monte Carlo estimator,
      applied through uniform hemisphere sampling and importance sampling of lights.
      <br><br>
      In Global illumination, our implementation focused on Sampling with Diffuse BSDF. We ensured that
      at_least_one_bounce_radiance calls one_bounce_radiance, and then recursively calls itself for rendering images
      with global illumination effects. We resolved an issue where the global illumination effect was not visible due to
      the incorrect initialization of camera rays' depths, which was fixed by setting them to max_ray_depth.
      <br><br>
      In Adaptive Sampling, we introduced the concept of confidence intervals to optimize sampling speed and efficiency.
      During sampling, we periodically compute I = 1.96 * (sigma / sqrt(n)) and check if I is less than or equal to
      maxTolerance * μ to determine if the pixel has converged. This approach involves calculating u = s1 / n and sigma²
      = (s2 – s1 * s1 / n) / (n - 1), where s1 and s2 represent the sum of samples and the sum of squared samples,
      respectively. Sampling is halted once convergence is achieved, optimizing the rendering process.
    </p>
    <br>

    <h2 align="middle">Part 1: Ray Generation and Scene Intersection (20 Points)</h2>
    <!-- Walk through the ray generation and primitive intersection parts of the rendering pipeline.
Explain the triangle intersection algorithm you implemented in your own words.
Show images with normal shading for a few small .dae files. -->

    <h3>
      Walk through the ray generation and primitive intersection parts of the rendering pipeline.
    </h3>
    <p>
      In the ray generation process, image coordinates from the image space are initially transformed into camera space.
      Here, rays are generated and subsequently converted into world space. Within camera space, the camera is
      positioned at the origin (0, 0, 0), oriented along the negative Z-axis – the view direction. The virtual camera
      sensor is situated in the Z = -1 plane. The sensor's bottom-left corner is located at (-tan(hFov/2), -tan(vFov/2),
      -1), and its top-right corner at (tan(hFov/2), tan(vFov/2), -1), with the center at (0, 0, -1). This sensor
      arrangement in the camera space corresponds to an image in image space, with a mapping system where the image
      coordinates (0, 0) align with the sensor's bottom-left corner, and (1, 1) with its top-right corner. The ray in
      camera space originates from the camera and passes through the point on the sensor mapped from the image space
      coordinates (x, y). Finally, this ray is transformed into world space by applying the camera-to-world rotation
      matrix (c2w) to the ray’s direction in camera space, and setting the ray’s origin to the camera's position in
      world space.
      <br><br>
      For primitive intersection, we integrate the equations of the ray and the primitive (a plane for Ray-Triangle
      Intersection, and a circle for Ray-Sphere Intersection) to solve for the ray's parameter 't'. A valid 't'
      indicates an intersection with the primitive. The ray equation is expressed as r(t) = o + td, where 0 ≤ t ≤
      infinity. The plane equation is defined as p: (p – p') * N = 0, and the sphere equation is p: (p – c)² – R² = 0.
    </p>
    <br>

    <h3>
      Explain the triangle intersection algorithm you implemented in your own words.
    </h3>
    <p>
      We begin by finding the equation of the plane on which the triangle lies. This is done by computing the
      multiplication of the plane's normal vector with a vector in the plane, resulting in the formula (p – p’) * N = 0.
      The normal vector is obtained by the cross product of two edge vectors of the triangle. We then integrate this
      plane equation with the ray’s equation to solve for the parameter 't', using t = (p’ - o) * N / (d * N). If 't' is
      outside the ray's min_t and max_t range, the intersection is invalid. Conversely, a valid 't' within the bounds
      necessitates a check to ensure the intersection point (denoted as p) is inside the triangle, defined by vertices
      v1, v2, and v3. If these vertices are not in a counterclockwise order, we reorder them accordingly. The cross
      product of v1 = (p – v1) x (v2 – v1), v2 = (p – v2) x (v3 – v2), and v3 = (p – v3) x (v1 – v3) is computed,
      followed by the dot products of v1 * v2 and v2 * v3. If both dot products are greater than 0, it confirms that p
      is within the triangle.
      <br><br>
      If the ray intersects with the triangle, we update the information in Intersection *isect. The intersection's 't'
      value, isect->t, is set to the ray's 't'. We then compute the normal vector of isect using Barycentric
      coordinates, calculated by determining the portion of the triangle areas pv1v2, pv2v3, and pv3v1 relative to the
      area of triangle v1v2v3, obtained through the cross product of the edge vectors and calling norm(). After
      computing alpha, beta, and gamma, the normal vector at the intersection, isect->n, is determined as alpha * n1 +
      beta * n2 + gamma * n3, where n1, n2, and n3 are the normals at vertices v1, v2, and v3, respectively. We then set
      isect->primitive to the current object and update isect->bsdf with get_bsdf().

    </p>
    <br>

    <h3>
      Show images with normal shading for a few small .dae files.
    </h3>
    <!-- Example of including multiple figures -->
    <div align="middle">
      <table style="width:100%">
        <tr align="center">
          <td>
            <img src="images/part1/CBempty_normal.png" align="middle" width="400px" />
            <figcaption>CBempty.dae</figcaption>
          </td>
          <td>
            <img src="images/part1/CBspheres_normal.png" align="middle" width="400px" />
            <figcaption>CBspheres.dae</figcaption>
          </td>
        </tr>
        <tr align="center">
          <td>
            <img src="images/part1/banana_normal.png" align="middle" width="400px" />
            <figcaption>banana.dae</figcaption>
          </td>
          <td>
            <img src="images/part1/building_normal.png" align="middle" width="400px" />
            <figcaption>building.dae</figcaption>
          </td>
        </tr>
      </table>
    </div>
    <br>


    <h2 align="middle">Part 2: Bounding Volume Hierarchy (20 Points)</h2>
    <!-- Walk through your BVH construction algorithm. Explain the heuristic you chose for picking the splitting point.
Show images with normal shading for a few large .dae files that you can only render with BVH acceleration.
Compare rendering times on a few scenes with moderately complex geometries with and without BVH acceleration. Present your results in a one-paragraph analysis. -->

    <h3>
      Walk through your BVH construction algorithm. Explain the heuristic you chose for picking the splitting point.
    </h3>
    <p>
      In constructing the Bounding Volume Hierarchy (BVH), we first determine the number of primitives by calculating
      'end - start'. We then approach this with two distinct cases based on whether the number of primitives is greater
      than 'max_leaf_size' or less than or equal to 'max_leaf_size'.
      <br><br>
      In the first scenario, where the primitive count exceeds 'max_leaf_size', we begin by expanding the bounding box
      of the internal node and computing the average centroid of the primitives. We then partition these primitives into
      two groups based on the x-coordinate of their centroids, using the std::partition() function. Primitives with
      centroids having an x-coordinate less than the average are grouped to the left, while the rest fall into the right
      group. The function's returned iterator marks the division between these groups, serving as the end iterator for
      the left child and the start iterator for the right child. In instances where one group ends up empty and the
      other contains all the primitives, we sort the primitives by their centroid's x-coordinate and allocate the first
      'max_leaf_size' number of primitives to the left child node, with the remaining assigned to the right child.
      Following this partition, we recursively invoke 'construct_bvh' for both the left and right child nodes. The
      current node, now fully constructed, is returned as a pointer.
      <br><br>
      In the second case, applicable when the number of primitives is within or equal to 'max_leaf_size', the node is
      treated as a leaf node. We expand its bounding box and set 'node->start = start' and 'node->end = end', allowing
      the leaf node to encompass the relevant primitives. This leaf node is then returned as a pointer.
      <br><br>
      Our heuristic is based on the premise that using the average centroid of the primitives effectively divides them
      into two balanced groups, thereby reducing the likelihood of all primitives falling into a single group. In the
      event this imbalance does occur, we mitigate it by assigning a 'max_leaf_size' number of nodes with the smallest
      x-coordinate centroids to the left child, and the remainder to the right child.
    </p>

    <h3>
      Show images with normal shading for a few large .dae files that you can only render with BVH acceleration.
    </h3>
    <!-- Example of including multiple figures -->
    <div align="middle">
      <table style="width:100%">
        <tr align="center">
          <td>
            <img src="images/part2/maxplanck_normal.png" align="middle" width="400px" />
            <figcaption>maxplanck.dae</figcaption>
          </td>
          <td>
            <img src="images/part2/CBlucy_normal.png" align="middle" width="400px" />
            <figcaption>CBlucy.dae</figcaption>
          </td>
        </tr>
        <tr align="center">
          <td>
            <img src="images/part2/CBdragon_normal.png" align="middle" width="400px" />
            <figcaption>CBdragon.dae</figcaption>
          </td>
          <td>
            <img src="images/part2/blob_normal.png" align="middle" width="400px" />
            <figcaption>blob.dae</figcaption>
          </td>
        </tr>
      </table>
    </div>
    <br>

    <h3>
      Compare rendering times on a few scenes with moderately complex geometries with and without BVH acceleration.
      Present your results in a one-paragraph analysis.
    </h3>
    <p>
      In our rendering tests, we observed significant performance differences when using the Bounding Volume Hierarchy
      (BVH) compared to not using it. For instance, rendering the 'cow.dae' file without BVH took 13.3022 seconds, but
      with BVH, it was drastically reduced to 0.5269 seconds. Similarly, the 'maxplanck.dae' file took 138.9985 seconds
      without BVH and only 1.3315 seconds with it. The most notable improvement was for the 'CBlucy.dae' file, where
      rendering time dropped from 465.9981 seconds without BVH to a mere 0.2980 seconds with BVH.
      <br><br>
      The reason behind these improvements is rooted in the computational efficiency of the BVH algorithm. Without BVH,
      the rendering process involves checking all primitives in the leaf (root) node if the ray intersects with the
      bounding box, leading to an O(n) time complexity. In contrast, with BVH, if the ray intersects the root node's
      bounding box, the algorithm recursively checks the bounding boxes of the left and right child nodes for
      intersections. Ideally, these child nodes split the primitives roughly in half, thus significantly reducing the
      number of primitives checked at each step. This recursive division continues down to the leaf nodes, circumventing
      the need to check every primitive initially. As a result, the BVH algorithm operates with a more efficient
      O(log(n)) time complexity.
    <div align="middle">
      <table style="width:100%">
        <tr align="center">
          <td>
            <img src="images/part2/cow_0.png" align="middle" width="400px" />
            <figcaption>cow.dae without BVH acceleration</figcaption>
          </td>
          <td>
            <img src="images/part2/cow_1.png" align="middle" width="400px" />
            <figcaption>cow.dae with BVH acceleration</figcaption>
          </td>
        </tr>
        <tr align="center">
          <td>
            <img src="images/part2/maxplanck_0.png" align="middle" width="400px" />
            <figcaption>maxplanck.dae without BVH acceleration</figcaption>
          </td>
          <td>
            <img src="images/part2/maxplanck_1.png" align="middle" width="400px" />
            <figcaption>maxplanck.dae with BVH acceleration</figcaption>
          </td>
        </tr>
        <tr align="center">
          <td>
            <img src="images/part2/CBlucy_0.png" align="middle" width="400px" />
            <figcaption>CBlucy.dae without BVH acceleration</figcaption>
          </td>
          <td>
            <img src="images/part2/CBlucy_1.png" align="middle" width="400px" />
            <figcaption>CBlucy.dae with BVH acceleration</figcaption>
          </td>
        </tr>
      </table>
    </div>
    <br>
    </p>
    <br>

    <h2 align="middle">Part 3: Direct Illumination (20 Points)</h2>
    <!-- Walk through both implementations of the direct lighting function.
Show some images rendered with both implementations of the direct lighting function.
Focus on one particular scene with at least one area light and compare the noise levels in soft shadows when rendering with 1, 4, 16, and 64 light rays (the -l flag) and with 1 sample per pixel (the -s flag) using light sampling, not uniform hemisphere sampling.
Compare the results between uniform hemisphere sampling and lighting sampling in a one-paragraph analysis. -->

    <h3>
      Walk through both implementations of the direct lighting function.
    </h3>
    <p>
      (1) Uniform Hemisphere Sampling
      <br><br>
      In this approach, we start by constructing a local coordinate system anchored at the intersection point, with the
      intersection normals aligned to the z-axis. We then calculate the origin and direction of the emitted light rays,
      including the direction of the sample rays from each light source. This involves computing the BRDF function value
      at the intersection point and the uniform PDF value of the sampled light.
      <br><br>
      Light rays are emitted in the direction of the sampled rays, and we check for their intersections with scene
      objects. If an intersection occurs, we calculate the corresponding radiance using the get_emission() function,
      weighing it with consideration for the angle and PDF. This process is repeated until we reach the predefined
      number of samples, after which we average all results to derive the final radiance estimate.
      <br><br>
      (2) Importance Sampling
      <br><br>
      This method is akin to uniform hemisphere sampling but with notable differences. When sampling each light source,
      we assess the sampled light's position and direction to ascertain if it is obstructed by objects in the scene. In
      this case, we prefer that no intersection occurs.
      <br><br>
      Importance sampling focuses on sampling each light source individually, unlike the hemisphere-wide sampling in the
      previous method. This means that the position and direction of the light source, ascertained through sampling, can
      be directly used for calculating direct lighting without the need for a coordinate system transformation.
      <br><br>
      Finally, the sample_L() function is utilized to sample and obtain the radiance, tailored to the specifics of the
      importance sampling technique.



    </p>

    <h3>
      Show some images rendered with both implementations of the direct lighting function.
    </h3>
    <p>Displayed below are images rendered using uniform hemisphere sampling. The image on the left was produced with
      settings of 4,4 for -s and -l, respectively, while the image on the right utilized settings of 16 and 8 for -s and
      -l.</p>
    <!-- Example of including multiple figures -->
    <div align="middle">
      <table style="width:100%">
        <!-- Header -->
        <tr align="center">
          <th>
            <b>Uniform Hemisphere Sampling</b>
          </th>
          <th>
            <b>Importance Sampling</b>
          </th>
        </tr>
        <br>
        <tr align="center">
          <td>
            <img src="images/part3/CBbunny_hemisphere_16_8.png" align="middle" width="400px" />
            <figcaption>CBbunny.dae</figcaption>
          </td>
          <td>
            <img src="images/part3/CBbunny_importance_16_8.png" align="middle" width="400px" />
            <figcaption>CBbunny.dae</figcaption>
          </td>
        </tr>
        <br>
        <tr align="center">
          <td>
            <img src="images/part3/CBspheres_lambertian_H_16_8.png" align="middle" width="400px" />
            <figcaption>CBspheres_lambertian.dae</figcaption>
          </td>
          <td>
            <img src="images/part3/CBspheres_lambertian_I_16_8.png" align="middle" width="400px" />
            <figcaption>CBspheres_lambertian.dae</figcaption>
          </td>
        </tr>
        <br>
      </table>
    </div>
    <br>

    <h3>
      Focus on one particular scene with at least one area light and compare the noise levels in <b>soft shadows</b>
      when rendering with 1, 4, 16, and 64 light rays (the -l flag) and with 1 sample per pixel (the -s flag) using
      light sampling, <b>not</b> uniform hemisphere sampling.
    </h3>
    <!-- Example of including multiple figures -->
    <div align="middle">
      <table style="width:100%">
        <tr align="center">
          <td>
            <img src="images/part3/bunny_l_1.png" align="middle" width="400px" />
            <figcaption>1 Light Ray (CBbunny.dae)</figcaption>
          </td>
          <td>
            <img src="images/part3/bunny_l_4.png" align="middle" width="400px" />
            <figcaption>4 Light Rays (CBbunny.dae)</figcaption>
          </td>
        </tr>
        <tr align="center">
          <td>
            <img src="images/part3/bunny_l_16.png" align="middle" width="400px" />
            <figcaption>16 Light Rays (CBbunny.dae)</figcaption>
          </td>
          <td>
            <img src="images/part3/bunny_l_16.png" align="middle" width="400px" />
            <figcaption>64 Light Rays (CBbunny.dae)</figcaption>
          </td>
        </tr>
      </table>
    </div>
    <p>
      The images clearly demonstrate that an increase in the -l value leads to a noticeable reduction in noise,
      resulting in clearer and smoother images. This improvement is attributed to the more comprehensive capture of
      light source variations achieved by using a greater number of light samples. Such enhanced sampling allows for
      smoother shadow transitions and more detailed lighting effects, contributing to a more realistic depiction in the
      images. Consequently, elevating the number of samples per light source substantially enhances the quality of the
      rendered images, effectively diminishing noise and enhancing clarity and smoothness.
    </p>
    <br>

    <h3>
      Compare the results between uniform hemisphere sampling and lighting sampling in a one-paragraph analysis.
    </h3>
    <p>
      Upon analyzing the images, we can discern that with an equal number of samples per pixel per unit light area,
      importance sampling yields superior results, producing smoother images with notably less noise. This enhanced
      performance stems from the fact that importance sampling prioritizes the light source's direction, focusing on
      capturing vital information about it. As a result, it effectively converges faster in rendering, concentrating on
      the most significant areas and thus reducing image noise more efficiently.
      <br><br>
      In contrast, uniform hemispherical area sampling might necessitate a higher sample count to reach similar levels
      of rendering quality. This is due to its tendency to distribute many samples in less relevant directions, which
      can lead to instability in the rendering process. Consequently, in practical applications, opting for light source
      sampling typically results in higher rendering efficiency and more stable outcomes, making it a preferred choice
      for achieving high-quality renders.
    </p>
    <br>


    <h2 align="middle">Part 4: Global Illumination (20 Points)</h2>
    <!-- Walk through your implementation of the indirect lighting function.
Show some images rendered with global (direct and indirect) illumination. Use 1024 samples per pixel.
Pick one scene and compare rendered views first with only direct illumination, then only indirect illumination. Use 1024 samples per pixel. (You will have to edit PathTracer::at_least_one_bounce_radiance(...) in your code to generate these views.)
For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2, 3, and 100 (the -m flag). Use 1024 samples per pixel.
Pick one scene and compare rendered views with various sample-per-pixel rates, including at least 1, 2, 4, 8, 16, 64, and 1024. Use 4 light rays.
You will probably want to use the instructional machines for the above renders in order to not burn up your own computer for hours. -->

    <h3>
      Walk through your implementation of the indirect lighting function.
    </h3>
    <p>
      In this task, we focus on implementing the at_least_one_bounce_radiation() function within the framework of path
      tracing. This function is crucial for calculating the radiation value following at least one reflection or
      refraction from an intersection point.
      <br><br>
      Our approach begins by invoking the one_bounce_radiation() function to compute the direct illumination
      contribution at the intersection point. Subsequently, the importance sampling method is employed to generate a new
      incident ray direction. This step is guided by the BSDF function and a sampler, from which we determine the
      sampling probability (pdf) and the value of the BSDF function.
      <br><br>
      The next phase involves using the Russian Roulette method to decide whether to continue the ray's trajectory. We
      calculate the probability of continuation (cpdf) and establish the minimum tracking distance. If the decision is
      to proceed, a new ray is constructed. Utilizing the BVH structure, an intersection operation is performed to find
      a new intersection point. Provided the conditions for continued tracking are met—namely, the tracking depth is
      greater than 1, and the new intersection point is valid—we recursively invoke the function to compute the indirect
      illumination achieved from the tracing process.
      <br><br>
      Ultimately, we obtain the radiation value after at least one reflection or refraction by aggregating the weighted
      sums of direct and indirect illumination. This result is then returned, providing a comprehensive measure of the
      radiation value in the context of path tracing.



    </p>
    <br>

    <h3>
      Show some images rendered with global (direct and indirect) illumination. Use 1024 samples per pixel.
    </h3>
    <!-- Example of including multiple figures -->
    <div align="middle">
      <table style="width:100%">
        <tr align="center">
          <td>
            <img src="images/part4/CBbunny_global.png" align="middle" width="400px" />
            <figcaption>CBbunny.dae</figcaption>
          </td>
          <td>
            <img src="images/part4/CBspheres_lambertian_global.png" align="middle" width="400px" />
            <figcaption>CBspheres_lambertian.dae</figcaption>
          </td>
        </tr>
      </table>
    </div>
    <br>

    <h3>
      Pick one scene and compare rendered views first with only direct illumination, then only indirect illumination.
      Use 1024 samples per pixel. (You will have to edit PathTracer::at_least_one_bounce_radiance(...) in your code to
      generate these views.)
    </h3>
    <!-- Example of including multiple figures -->
    <div align="middle">
      <table style="width:100%">
        <tr align="center">
          <td>
            <img src="images/part4/CBbunny_direct.png" align="middle" width="400px" />
            <figcaption>Only direct illumination (CBbunny.dae)</figcaption>
          </td>
          <td>
            <img src="images/part4/CBbunny_indirect.png" align="middle" width="400px" />
            <figcaption>Only indirect illumination (CBbunny.dae)</figcaption>
          </td>
        </tr>
      </table>
    </div>
    <br>
    <p>
      From our observations, using only direct illumination results in outcomes similar to those seen in Part 3,
      characterized predominantly by large shadows. This is primarily due to the lack of consideration for light bounces
      from other objects. On the other hand, if we rely solely on indirect illumination, the scene disregards the direct
      light from the light sources. This approach results in rendering the light source on the ceiling as black, and
      other areas of the scene appear quite dim. This dimness arises because these areas are illuminated exclusively by
      light that has been refracted or reflected off other objects, lacking any direct light contribution.

    </p>
    <br>

    <h3>
      For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2, 3, and 100 (the -m flag). Use 1024
      samples per pixel.
    </h3>
    <!-- Example of including multiple figures -->
    <div align="middle">
      <table style="width:100%">
        <tr align="center">
          <td>
            <img src="images/part4/bunny_m_0.png" align="middle" width="400px" />
            <figcaption>max_ray_depth = 0 (CBbunny.dae)</figcaption>
          </td>
          <td>
            <img src="images/part4/bunny_m_1.png" align="middle" width="400px" />
            <figcaption>max_ray_depth = 1 (CBbunny.dae)</figcaption>
          </td>
        </tr>
        <tr align="center">
          <td>
            <img src="images/part4/bunny_m_2.png" align="middle" width="400px" />
            <figcaption>max_ray_depth = 2 (CBbunny.dae)</figcaption>
          </td>
          <td>
            <img src="images/part4/bunny_m_3.png" align="middle" width="400px" />
            <figcaption>max_ray_depth = 3 (CBbunny.dae)</figcaption>
          </td>
        </tr>
        <tr align="center">
          <td>
            <img src="images/part4/bunny_m_100.png" align="middle" width="400px" />
            <figcaption>max_ray_depth = 100 (CBbunny.dae)</figcaption>
          </td>
        </tr>
      </table>
    </div>
    <br>
    <p>
      The images clearly demonstrate an increase in brightness correlating with higher max_depth values. At depth 0, the
      first image in the sequence, the scene is notably dark. This darkness is largely due to the minimal bouncing of
      rays, resulting in an abundance of shadows. However, as we progress to depths 2 and 3, there is a significant
      brightening of the rabbit. This enhancement is attributed to the addition of multiple layers of indirect
      illumination, where the rabbit is illuminated by light reflected and refracted from surrounding objects. At a
      depth of 100, the impact of indirect illumination reaches near convergence, reflecting what can be considered the
      final global illumination effect. This depth presents a much brighter and more evenly illuminated scene.
    </p>
    <br>

    <h3>
      Pick one scene and compare rendered views with various sample-per-pixel rates, including at least 1, 2, 4, 8, 16,
      64, and 1024. Use 4 light rays.
    </h3>
    <!-- Example of including multiple figures -->
    <div align="middle">
      <table style="width:100%">
        <tr align="center">
          <td>
            <img src="images/part4/CBbunny_global_s_1.png" align="middle" width="400px" />
            <figcaption>1 sample per pixel (CBbunny.dae)</figcaption>
          </td>
          <td>
            <img src="images/part4/CBbunny_global_s_2.png" align="middle" width="400px" />
            <figcaption>2 samples per pixel (CBbunny.dae)</figcaption>
          </td>
        </tr>
        <tr align="center">
          <td>
            <img src="images/part4/CBbunny_global_s_4.png" align="middle" width="400px" />
            <figcaption>4 samples per pixel (CBbunny.dae)</figcaption>
          </td>
          <td>
            <img src="images/part4/CBbunny_global_s_8.png" align="middle" width="400px" />
            <figcaption>8 samples per pixel (CBbunny.dae)</figcaption>
          </td>
        </tr>
        <tr align="center">
          <td>
            <img src="images/part4/CBbunny_global_s_16.png" align="middle" width="400px" />
            <figcaption>16 samples per pixel (CBbunny.dae)</figcaption>
          </td>
          <td>
            <img src="images/part4/CBbunny_global_s_64.png" align="middle" width="400px" />
            <figcaption>64 samples per pixel (CBbunny.dae)</figcaption>
          </td>
        </tr>
        <tr align="center">
          <td>
            <img src="images/part4/CBbunny_global_s_1024.png" align="middle" width="400px" />
            <figcaption>1024 samples per pixel (CBbunny.dae)</figcaption>
          </td>
        </tr>
      </table>
    </div>
    <br>
    <p>
      As the sample-per-pixel rate increases, a clear enhancement in image quality is observed, characterized by reduced
      noise and a smoother overall appearance. However, it's noteworthy that there is no substantial change in either
      the brightness or the shadows of the image. This phenomenon occurs because adjusting the sample-per-pixel rate
      affects only the sampling frequency, not the maximum depth of ray bounces, which contrasts with the parameters
      altered in the previous section. Consequently, since the indirect lighting component remains unaltered, the
      brightness of the image stays consistent regardless of the changes in sampling rate.

    </p>
    <br>


    <h2 align="middle">Part 5: Adaptive Sampling (20 Points)</h2>
    <!-- Explain adaptive sampling. Walk through your implementation of the adaptive sampling.
Pick one scene and render it with at least 2048 samples per pixel. Show a good sampling rate image with clearly visible differences in sampling rate over various regions and pixels. Include both your sample rate image, which shows your how your adaptive sampling changes depending on which part of the image you are rendering, and your noise-free rendered result. Use 1 sample per light and at least 5 for max ray depth. -->

    <h3>
      Explain adaptive sampling. Walk through your implementation of the adaptive sampling.
    </h3>
    <p>
      This approach utilizes confidence intervals to assess the quality of pixel sampling in rendering. By calculating
      these intervals, it evaluates both the variance and the confidence level of the rendering results. This method
      effectively identifies which pixels require additional sampling to reduce variance and increase confidence,
      thereby enhancing image quality. Simultaneously, it also pinpoints pixels where the number of samples can be
      reduced, conserving computational resources.
      <br><br>
      In adaptive sampling, the number of samples progressively increases and stops when certain criteria are met. For
      this task, sampling ceases when the variance of the sampling results falls below a predetermined tolerance level,
      indicating sufficient accuracy.
      <br><br>
      At each sampling point, the generate_ray() function is employed to produce camera rays. The
      est_radiance_global_illumination() function then tracks these rays and accumulates the results. The number of
      samples is continuously tallied, and upon reaching multiples of the batch size (samplesPerBatch), we compute the
      average and variance of the results. The decision to stop sampling immediately is based on these calculations and
      the set tolerance.
      <br><br>
      Ultimately, we average all sampling outcomes, update these results to the sample buffer, and document the number
      of samples for each pixel. This process ensures an efficient rendering approach that balances quality with
      computational economy.

    </p>
    <br>

    <h3>
      Pick two scenes and render them with at least 2048 samples per pixel. Show a good sampling rate image with clearly
      visible differences in sampling rate over various regions and pixels. Include both your sample rate image, which
      shows your how your adaptive sampling changes depending on which part of the image you are rendering, and your
      noise-free rendered result. Use 1 sample per light and at least 5 for max ray depth.
    </h3>
    <!-- Example of including multiple figures -->
    <div align="middle">
      <table style="width:100%">
        <tr align="center">
          <td>
            <img src="images/part5/CBbunny_2048.png" align="middle" width="400px" />
            <figcaption>Rendered image (CBbunny.dae)</figcaption>
          </td>
          <td>
            <img src="images/part5/CBbunny_2048_rate.png" align="middle" width="400px" />
            <figcaption>Sample rate image (CBbunny.dae)</figcaption>
          </td>
        </tr>
        <tr align="center">
          <td>
            <img src="images/part5/CBspheres_lambertian_2048.png" align="middle" width="400px" />
            <figcaption>Rendered image (CBspheres_lambertian.dae)</figcaption>
          </td>
          <td>
            <img src="images/part5/CBspheres_lambertian_2048_rate.png" align="middle" width="400px" />
            <figcaption>Sample rate image (CBspheres_lambertian.dae)</figcaption>
          </td>
        </tr>
      </table>
    </div>
    <p>
      Adaptive sampling effectively enables dynamic adjustment of the number of samples, significantly enhancing
      rendering speed without compromising on the quality of the results. This method not only reduces noise but also
      contributes to a smoother appearance in the final images.
    </p>
    <br>


</body>

</html>